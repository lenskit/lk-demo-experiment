{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdd6215",
   "metadata": {},
   "source": [
    "# Evaluation Analysis for Recommender Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a320cc",
   "metadata": {},
   "source": [
    "In this section we will analyze and compare the generated recommendations and predictions from a predefined list of algorithms with the goal of assessing the performance of each algorithm with respect to a metric. In other words, we would rank the algorithms for each metric considered with respect to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e036a6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33745d6b",
   "metadata": {},
   "source": [
    "Below are the list of packages required to successfully run the analysis. They are divided into partitions to signify their specific task.<br>\n",
    "We need the pathlib package for working with files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0165c",
   "metadata": {},
   "source": [
    "We would use the pandas for analyzing and manipulating our data while seaborn and matplotlib are used for data visualization. statsmodels.graphics.gofplots and scipy.stats.shapiro are used for normality check. Scipy.stats.friedmanchisquare is a non-parametric test used to determine the statistical significance in metric results and the wilcoxon test is used for pairwise comparison of sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a2561",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8e240",
   "metadata": {},
   "source": [
    "Import the LensKit metrics for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2284e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit.data import ItemListCollection, UserIDKey\n",
    "from lenskit.metrics import RunAnalysis, RMSE, NDCG, RecipRank, RBP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12975d2a",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The recommendations are in `runs`, and we will need to reassemble the test data from `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d3a801",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"ml100k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root = Path(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c36d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [fld for fld in output_root.glob(f'{dataset}-*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786015de",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = ItemListCollection(['algorithm', 'user_id'], index=False)\n",
    "for fld in dirs:\n",
    "    for file in fld.glob(\"recs-*\"):\n",
    "        rec = pd.read_parquet(file)\n",
    "        rec = ItemListCollection.from_df(rec, UserIDKey)\n",
    "        recs.add_from(rec, algorithm=fld.name.split(\"-\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_algos = sorted(set(a for (a, _u) in recs.keys()))\n",
    "rec_algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fcc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ItemListCollection(['algorithm', 'user_id'], index=False)\n",
    "for fld in dirs:\n",
    "    for file in fld.glob(\"pred-*\"):\n",
    "        pred = pd.read_parquet(file)\n",
    "        pred = ItemListCollection.from_df(pred, UserIDKey)\n",
    "        preds.add_from(pred, algorithm=fld.name.split(\"-\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29697c2",
   "metadata": {},
   "source": [
    "We need to load the test data so that we have the ground truths for computing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f09915",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_root = Path(\"data-split\")\n",
    "split_dir = split_root / dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ItemListCollection(UserIDKey)\n",
    "for file in split_dir.glob(\"test-*.parquet\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    test.add_from(ItemListCollection.from_df(df, UserIDKey))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44840f3c",
   "metadata": {},
   "source": [
    "## Top-N Metrics\n",
    "\n",
    "`RunListAnalysis` computes metrics for recommendation results and takes care of\n",
    "matching recommendations and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = RunAnalysis()\n",
    "\n",
    "ra.add_metric(NDCG())\n",
    "ra.add_metric(RecipRank())\n",
    "ra.add_metric(RBP())\n",
    "\n",
    "results = ra.compute(recs, test)\n",
    "results.list_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf0a3c",
   "metadata": {},
   "source": [
    "We can reshape the list metrics and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c83aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = results.list_metrics()\n",
    "metrics = metrics.melt(var_name='metric', ignore_index=False).reset_index()\n",
    "sns.catplot(metrics, x='algorithm', y='value', col='metric', kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27389c",
   "metadata": {},
   "source": [
    "## Prediction RMSE\n",
    "\n",
    "We will also look at the prediction RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = RunAnalysis()\n",
    "\n",
    "ra.add_metric(RMSE(missing_scores='ignore', missing_truth='ignore'))\n",
    "\n",
    "results = ra.compute(preds, test)\n",
    "results.list_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(results.list_metrics().reset_index(), x='algorithm', y='RMSE', kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660590f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "notebook_metadata_filter": "split_at_heading"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "split_at_heading": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
