{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5b6343",
   "metadata": {},
   "source": [
    "# Evaluation Analysis for Recommender Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6041b3e",
   "metadata": {},
   "source": [
    "In this section we will analyze and compare the generated recommendations and predictions from a predefined list of algorithms with the goal of assessing the performance of each algorithm with respect to a metric. In other words, we would rank the algorithms for each metric considered with respect to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45550cf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be94ff",
   "metadata": {},
   "source": [
    "Below are the list of packages required to successfully run the analysis. They are divided into partitions to signify their specific task.<br>\n",
    "We need the pathlib package for working with files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee17d8",
   "metadata": {},
   "source": [
    "We would use the pandas for analyzing and manipulating our data while seaborn and matplotlib are used for data visualization. statsmodels.graphics.gofplots and scipy.stats.shapiro are used for normality check. Scipy.stats.friedmanchisquare is a non-parametric test used to determine the statistical significance in metric results and the wilcoxon test is used for pairwise comparison of sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9436161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec24e66",
   "metadata": {},
   "source": [
    "Logging to show what's happening in LensKit routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit import util\n",
    "util.log_to_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "_log = logging.getLogger('eval-report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc992e",
   "metadata": {},
   "source": [
    "We will use lenskit for training, running, and evaluating recommender algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634390f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit import topn\n",
    "from lenskit.metrics.predict import rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e481be5",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d674ab",
   "metadata": {},
   "source": [
    "We specify the dataset we will use for our analysis and the main directory from where we read the recommendation and prediction files. From the main directory we find all the directories associated with the dataset and then read the recommendation and predictions files from those directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da4371",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"ml100k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e538c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root = Path(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [fld for fld in output_root.glob(f'{dataset}-*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa629c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "for fld in dirs: \n",
    "    for file in fld.glob(\"recs-*\"):\n",
    "        rec = pd.read_csv(file, sep=',')\n",
    "        rec[\"algorithm\"] = fld.name.split(\"-\")[1]\n",
    "        recs.append(rec)\n",
    "\n",
    "recs = pd.concat(recs, ignore_index=True)\n",
    "recs = recs.astype({'algorithm': 'category'})\n",
    "recs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_algos = recs['algorithm'].unique()\n",
    "rec_algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for fld in dirs:\n",
    "    for file in fld.glob(\"pred-*\"):\n",
    "        pred = pd.read_csv(file, sep=',')\n",
    "        pred[\"algorithm\"] = fld.name.split(\"-\")[1]\n",
    "        preds.append(pred)\n",
    "\n",
    "preds = pd.concat(preds, ignore_index=True)\n",
    "preds = preds.astype({'algorithm': 'category'})\n",
    "preds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf63a44",
   "metadata": {},
   "source": [
    "We need to load the test data so that we have the ground truths for computing accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_root = Path(\"data-split\")\n",
    "split_dir = split_root / dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for file in split_dir.glob(\"test-*.csv.gz\"):\n",
    "    test.append(pd.read_csv(file, sep=',', index_col='index').assign(part=file.stem.replace('.csv', '')))\n",
    "\n",
    "test = pd.concat(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cefb23",
   "metadata": {},
   "source": [
    "# Top-N Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d9089",
   "metadata": {},
   "source": [
    "The topn.RecListAnalysis class computes top-N metrics for recommendation list and takes care of making sure that the recommendations and ground truths are properly matched. Refer to the documentation for detailed explanation of the purpose for the RecListAnalysis class and how the analysis is done - https://lkpy.lenskit.org/en/stable/evaluation/topn-metrics.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08638d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rla = topn.RecListAnalysis()\n",
    "\n",
    "rla.add_metric(topn.precision)\n",
    "rla.add_metric(topn.recip_rank)\n",
    "rla.add_metric(topn.ndcg)\n",
    "results = rla.compute(recs, test, include_missing=True)\n",
    "results = results.fillna(0)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748010c",
   "metadata": {},
   "source": [
    "We will reshape the 'results' dataframe by stacking the columns to index and then use the bar chart to visualize the performance of our algorithms with respect to the precision, reciprocal rank and ndcg metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pltData = (results.drop(columns=['nrecs', 'ntruth']).stack()).reset_index()\n",
    "pltData.columns = ['algorithm', 'user', 'metric', 'val']\n",
    "pltData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e209e7f",
   "metadata": {},
   "source": [
    "We need to determine if the differences we observe in the performances of the algorithms for the various metrics are statistically significant. To achieve this, we will need to use either a parametric or non-parametric statistical test for comparing the differences. We will consider a parametric test - repeated ANOVA measure cause our sample groups are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb05de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x = \"algorithm\", y = \"val\", data = pltData, kind=\"bar\", col = \"metric\", aspect=1.2, height=3, sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04660e02",
   "metadata": {},
   "source": [
    "### Statistical Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b9e26",
   "metadata": {},
   "source": [
    "We will now examine these metrics in more detail and test for statistical significance.  Best practice for statistical testing for recommender systems is still a work in progress, but this method avoids most of the validity problems we know about.\n",
    "\n",
    "The Friedman $\\chi^2$ test will test the null hypothesis that there is no difference between algorithms in terms of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394dcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_friedman(metric):\n",
    "    groups = [df[metric] for a, df in results.groupby('algorithm')]\n",
    "\n",
    "    stat, p = friedmanchisquare(*groups)\n",
    "    return pd.Series({'stat': stat, 'p': p}, name=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d3c48",
   "metadata": {},
   "source": [
    "If the test detects significance, we have at least one pair of different algorithms - but which pairs?  Unfortunately, the good tests aren't readily available in scipy and friends, so we're going to take a conservative approch and perform pairwise Wilconxon signed-rank tests with a Bonferroni adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baafd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame.from_records(combinations(rec_algos, 2), columns=['A1', 'A2'])\n",
    "def am_wilcox(metric, p_scale = len(pairs)):\n",
    "    def compute(df):\n",
    "        vs1 = results.loc[df['A1'], metric]\n",
    "        vs2 = results.loc[df['A2'], metric]\n",
    "        diff = (vs1 - vs2).mean()\n",
    "        stat, p = wilcoxon(vs1, vs2)\n",
    "        return pd.Series({'diff': diff, 'stat': stat, 'p': p, 'adj_p': p * p_scale})\n",
    "    scores = pairs.apply(compute, axis=1)\n",
    "    return pairs.join(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2171b",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_friedman('precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e515964",
   "metadata": {},
   "source": [
    "This indicates a statistically significant difference exists - but where is it?\n",
    "\n",
    "To answer that question, we need a post-hoc test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c44097",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_wilcox('precision').sort_values('diff', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c5811",
   "metadata": {},
   "source": [
    "We can consider pairs with an adjusted $p$ value less than 0.05 (5.0e-2) to be statistically significant.  However, we have so many data points, the substance of significance is questionable.  Pay attention to the 'diff' column, which is the difference between A1 and A2 (how much A1 outperforms A2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c3a89",
   "metadata": {},
   "source": [
    "### nDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10456961",
   "metadata": {},
   "source": [
    "Now let's do the same for nDCG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cfd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_friedman('ndcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_wilcox('ndcg').sort_values('diff', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e7517",
   "metadata": {},
   "source": [
    "### Reciprocal Rank\n",
    "\n",
    "We'll do this also for reciprocal rank (what is aggregated to compute MRR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73974759",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_friedman('recip_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cded846",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_wilcox('recip_rank').sort_values('diff', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9421c0",
   "metadata": {},
   "source": [
    "## Prediction RMSE\n",
    "\n",
    "We will also look at the prediction RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53193c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rmse = preds.groupby(['algorithm', 'user']).apply(lambda df: rmse(df['prediction'], df['rating']))\n",
    "user_rmse = user_rmse.reset_index(name='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='algorithm', y='RMSE', data=user_rmse, kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1d2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "split_at_heading"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "split_at_heading": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
