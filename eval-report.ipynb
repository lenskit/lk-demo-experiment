{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f141a2",
   "metadata": {},
   "source": [
    "# Evaluation Analysis for Recommender Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62da5d",
   "metadata": {},
   "source": [
    "In this section we will analyze and compare the generated recommendations and predictions from a predefined list of algorithms with the goal of assessing the performance of each algorithm with respect to a metric. In other words, we would rank the algorithms for each metric considered with respect to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d5fb5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf426238",
   "metadata": {},
   "source": [
    "Below are the list of packages required to successfully run the analysis. They are divided into partitions to signify their specific task.<br>\n",
    "We need the pathlib package for working with files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e42f1",
   "metadata": {},
   "source": [
    "Load libraries for analysis and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b33c3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d0984",
   "metadata": {},
   "source": [
    "Import the LensKit metrics for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit.data import Dataset, ItemListCollection\n",
    "from lenskit.metrics import RunAnalysis, RMSE, NDCG, RecipRank, RBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lkdemo.datasets import split_fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f34e1c",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The recommendations are in `runs`, and we will need to reassemble the test data from `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67986a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"ml-100k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root = Path(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [fld for fld in output_root.glob(f'{dataset}-*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = ItemListCollection(['model', 'user_id'], index=False)\n",
    "for fld in dirs:\n",
    "    for file in fld.glob(\"recs-*\"):\n",
    "        rec = ItemListCollection.load_parquet(file)\n",
    "        recs.add_from(rec, model=fld.name.split(\"-\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_algos = sorted(set(a for (a, _u) in recs.keys()))\n",
    "rec_algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36504c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ItemListCollection(['model', 'user_id'], index=False)\n",
    "for fld in dirs:\n",
    "    for file in fld.glob(\"pred-*\"):\n",
    "        pred = ItemListCollection.load_parquet(file)\n",
    "        preds.add_from(pred, model=fld.name.split(\"-\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac7000",
   "metadata": {},
   "source": [
    "We need to load the test data so that we have the ground truths for computing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14649b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load(f\"data/{dataset}\")\n",
    "split = split_fraction(data, 0.2)\n",
    "test = split.test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12706408",
   "metadata": {},
   "source": [
    "And identify users in the training set, so we only report metrics over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = split.train.user_stats()\n",
    "train_users = train_users[train_users['rating_count'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cd787",
   "metadata": {},
   "source": [
    "## Top-N Metrics\n",
    "\n",
    "`RunListAnalysis` computes metrics for recommendation results and takes care of\n",
    "matching recommendations and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a041f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = RunAnalysis()\n",
    "\n",
    "ra.add_metric(NDCG())\n",
    "ra.add_metric(RecipRank())\n",
    "ra.add_metric(RBP())\n",
    "\n",
    "rec_results = ra.compute(recs, test)\n",
    "rec_results.list_summary('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c724915",
   "metadata": {},
   "source": [
    "We can reshape the list metrics and plot them, after filtering to only users with at least 1 training rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = rec_results.list_metrics()\n",
    "metrics = metrics.melt(var_name='metric', ignore_index=False).reset_index()\n",
    "metrics = metrics[metrics['user_id'].isin(train_users.index)]\n",
    "sns.catplot(metrics, y='model', x='value', col='metric', kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a83f3",
   "metadata": {},
   "source": [
    "Let's look at the influence of training ratings on performance, clamping 15+\n",
    "into a single category â€” this helps understand perhaps surprising performance\n",
    "relative to cross-fold evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7498c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcounts = split.train.user_stats()['rating_count'].copy()\n",
    "tcounts[tcounts > 15] = 15\n",
    "metrics = rec_results.list_metrics().reset_index().join(tcounts, on='user_id')\n",
    "sns.lineplot(metrics, x='rating_count', y='NDCG', hue='model', errorbar='ci')\n",
    "plt.xlabel('# of Training Ratings')\n",
    "rc_ticks = np.arange(0, 16, 3)\n",
    "plt.xticks(rc_ticks, rc_ticks[:-1].tolist() + ['15+'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7a0c2",
   "metadata": {},
   "source": [
    "## Prediction RMSE\n",
    "\n",
    "We will also look at the prediction RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = RunAnalysis()\n",
    "\n",
    "pa.add_metric(RMSE(missing_scores='ignore', missing_truth='ignore'))\n",
    "\n",
    "pred_results = pa.compute(preds, test)\n",
    "pred_results.list_summary('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_metrics = pred_results.list_metrics().reset_index()\n",
    "pred_metrics = pred_metrics[pred_metrics['user_id'].isin(train_users.index)]\n",
    "sns.catplot(pred_metrics, x='model', y='RMSE', kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e51c4",
   "metadata": {},
   "source": [
    "## Save Metrics\n",
    "\n",
    "We'll now save the metrics to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d422333",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlsum = rec_results.list_summary('model')['mean'].unstack()\n",
    "rlsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlsum.to_json(f'eval-metrics.{dataset}.json', orient='index')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "notebook_metadata_filter": "split_at_heading"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "split_at_heading": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
